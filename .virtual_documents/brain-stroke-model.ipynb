


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, LabelEncoder 
from sklearn.linear_model import LogisticRegression
from sklearn import svm 
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB 
from sklearn.metrics import accuracy_score , classification_report








df_Stroke =pd.read_csv("/kaggle/input/brain-stroke-dataset/brain_stroke.csv")



df_Stroke.shape 


df_Stroke.columns


df_Stroke.head()


df_Stroke.info()



df_Stroke.describe()


df_Stroke.duplicated().sum()


df_Stroke.isnull().sum()


df_Stroke['stroke'].replace({0: 'The patient did not have a stroke', 1: 'The patient had a stroke'}, inplace=True)


df_Stroke.head()


df_Stroke['stroke'].value_counts()





df_cat=df_Stroke[['gender', 'hypertension', 'heart_disease', 'ever_married','work_type', 'Residence_type', 
                 'smoking_status', 'stroke']]


for i in df_cat.columns:
    plt.figure(figsize = (15,6))
    sns.countplot(x=df_cat[i], data = df_cat, palette = 'hls')
    plt.xticks(rotation = 90)
    plt.show()






plt.figure(figsize=(8, 5))
palette = sns.color_palette("Set2")
sns.histplot(df_Stroke['age'], kde=True, color=palette[0])
plt.xticks(rotation=90, ha="right")
plt.show()






my_palette = ["#FF62D2", "#43AEF4"]

plt.figure(figsize=(20,10))

sns.displot(data=df_Stroke, x="age", hue="stroke", kind="kde",palette=my_palette)






plt.figure(figsize=(20,10))
palette = sns.color_palette(["#BA0ADA" , "#E20C7E"])
sns.displot(data=df_Stroke, x="age", hue="stroke",bins=20, palette=palette)
plt.xticks(rotation=45, ha="right")


numerical_data = df_Stroke[['age','avg_glucose_level','bmi']]
sns.kdeplot(data=numerical_data)





plt.figure(figsize=(12,8))
palette = sns.color_palette("Set2")
sns.displot(data=df_Stroke, x="work_type", hue="stroke",bins=20, palette=palette)
plt.xticks(rotation=45, ha="right")






['formerly smoked' 'never smoked' 'smokes' 'Unknown']
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
for col in df_Stroke.columns:
    if df_Stroke[col].dtype=='object':
        df_Stroke[col]=le.fit_transform(df_Stroke[col])


df_Stroke.head()





from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
for col_name in df_Stroke.columns:
    if df_Stroke[col_name].nunique() > 5:
        df_Stroke[col_name] = scaler.fit_transform(df_Stroke[[col_name]])





X = df_Stroke.drop("stroke",axis=1)
y =df_Stroke['stroke']


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,shuffle=True)






logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)

print('Logistic Regression accuracy score with all the features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)*100))
print(classification_report(y_test, y_pred))





dec_clf=DecisionTreeClassifier(criterion='gini',random_state=3,max_depth=5)
dec_clf.fit(X_train,y_train)
y_pred=dec_clf.predict(X_test)
print('DecisionTreeClassifier accuracy score:', dec_clf.score(X_test, y_test)*100)
print(classification_report(y_test, y_pred))





rand_clf=RandomForestClassifier(n_estimators=20,criterion="entropy")
rand_clf.fit(X_train,y_train)
y_pred=rand_clf.predict(X_test)
print('RandomForestClassifier accuracy score:', rand_clf.score(X_test, y_test)*100)
print(classification_report(y_test, y_pred))






model=svm.SVC(kernel='rbf',C=1,gamma=0.1)
model.fit(X_train,y_train)
y_pred=model.predict(X_test)
print('Accuracy for  SVM is ',metrics.accuracy_score(y_pred,y_test)*100)
print(classification_report(y_test, y_pred))





NaiveBayes=GaussianNB()
NaiveBayes.fit(X_train,y_train)
y_pred=model.predict(X_test)
print('The accuracy of the NaiveBayes is',metrics.accuracy_score(y_pred,y_test))
print(classification_report(y_test, y_pred))





from sklearn.ensemble import BaggingClassifier
deci_clf=RandomForestClassifier()
clf_bagging=BaggingClassifier(estimator=deci_clf ,n_estimators=20,random_state=42)
clf_bagging.fit(X_train,y_train)
y_pred=clf_bagging.predict(X_test)
print('The accuracy of the bagging classifier is',metrics.accuracy_score(y_pred,y_test))






from sklearn.ensemble import AdaBoostClassifier
decs_clf=RandomForestClassifier()
clf_boosting=AdaBoostClassifier(estimator=decs_clf ,n_estimators=20 ,random_state=42)
clf_boosting.fit(X_train,y_train)
y_pred=clf_boosting.predict(X_test)
print('The accuracy of the Boosting classifier is',metrics.accuracy_score(y_pred,y_test))






from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import cross_val_score #score evaluation
from sklearn.model_selection import cross_val_predict #prediction
ensemble_lin_rbf=VotingClassifier(estimators=[
                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),
                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),
                                              ('LR',LogisticRegression(C=0.05)),
                                              ('DT',DecisionTreeClassifier(random_state=0)),
                                              ('NB',GaussianNB()),
                                              ('svm',svm.SVC(kernel='linear',probability=True))
                                             ], 
                       voting='soft').fit(X_train,y_train)
print('The accuracy for ensembled model is:',ensemble_lin_rbf.score(X_test,y_test))
cross=cross_val_score(ensemble_lin_rbf,X,y, cv = 10,scoring = "accuracy")
print('The cross validated score is :',cross.mean())





from sklearn.model_selection import KFold #for K-fold cross validation
kfold = KFold(n_splits=10) # k=10, split the data into 10 equal parts
cv_mean=[]
accuracy=[]
std=[]
classifiers=['Linear Svm','Radial Svm','Logistic Regression','Decision Tree','Naive Bayes','Random Forest']
models=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]
for i in models:
    model = i
    cv_result = cross_val_score(model,X,y, cv = kfold,scoring = "accuracy")
    cv_result=cv_result
    cv_mean.append(cv_result.mean())
    std.append(cv_result.std())
    accuracy.append(cv_result)
new_models_dataframe2=pd.DataFrame({'CV Mean':cv_mean,'Std':std},index=classifiers)       
new_models_dataframe2


new_models_dataframe2['CV Mean'].plot.barh(width=0.8)
plt.title('Average CV Mean Accuracy')
fig=plt.gcf()
fig.set_size_inches(8,5)
plt.show()

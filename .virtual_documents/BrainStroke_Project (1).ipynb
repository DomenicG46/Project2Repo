import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from statistics import mean, stdev
from sklearn import preprocessing


#Ignoring future warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


# Load dataset
data = pd.read_csv("brain_stroke.csv")
data.head(5)





missing_values = data.isnull().sum()
print(missing_values)





# Check data types of each column
print(data.dtypes)


# 2. Encoding categorical variables
label_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
for col in label_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])


data


numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
min_values = data[numerical_cols].min()
max_values = data[numerical_cols].max()

# Calculate the range
range_values = max_values - min_values

# Combine results into a DataFrame for better visualization
range_df = pd.DataFrame({
    'Min': min_values,
    'Max': max_values,
    'Range': range_values
})

print(range_df)


# 3. Feature Scaling (for continuous variables)
scaler = StandardScaler()
data[['age', 'avg_glucose_level', 'bmi']] = scaler.fit_transform(data[['age', 'avg_glucose_level', 'bmi']])


# Descriptive statistics for numerical features
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
print("Descriptive statistics for numerical features:")
print(data[numerical_cols].describe())

# Visualizing distributions of numerical features
for col in numerical_cols:
    plt.figure(figsize=(8, 5))
    sns.histplot(data[col], bins=30, kde=True)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()


# Check for imbalance in the target variable
plt.figure(figsize=(8, 5))
sns.countplot(x='stroke', data=data)
plt.title("Distribution of Stroke")
plt.xlabel("Stroke")
plt.ylabel("Count")
plt.xticks([0, 1], ['No Stroke', 'Stroke'])
plt.show()


# Dropping stroke
X = data.drop('stroke', axis=1)
y = data['stroke']


X





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
lr = LogisticRegression()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    lr.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(lr.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))





X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)





# Logistic Regression
lr = LogisticRegression()
lr.fit(X_train, y_train)
y_pred_lr = lr.predict(X_test)
print("Logistic Regression")
print(classification_report(y_test, y_pred_lr))
print(confusion_matrix(y_test, y_pred_lr))






# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
kc = KNeighborsClassifier()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    kc.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(kc.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))









# K-Nearest Neighbors

kc.fit(X_train, y_train)
y_pred_kc = kc.predict(X_test)

print("\nK-Nearest Neighbors(Train-Test Split)")
print(classification_report(y_test, y_pred_kc))
print(confusion_matrix(y_test, y_pred_kc))





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
Rc = RandomForestClassifier()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    Rc.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(Rc.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))









# Random Forest
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)
print("Random Forest")
print(classification_report(y_test, y_pred_rf))
print(confusion_matrix(y_test, y_pred_rf))






# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
Gc = GradientBoostingClassifier()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    Gc.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(Gc.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))





# Gradient Boosting Classifier
gb = GradientBoostingClassifier()
gb.fit(X_train, y_train)
y_pred_gb = gb.predict(X_test)
print("Gradient Boosting Classifier")
print(classification_report(y_test, y_pred_gb))
print(confusion_matrix(y_test, y_pred_gb))





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
DT = DecisionTreeClassifier()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    DT.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(DT.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))





# Decision Tree Classifier
DT = DecisionTreeClassifier()
DT.fit(X_train, y_train)
y_pred_DT = DT.predict(X_test)
print("Decision Tree Classifier")
print(classification_report(y_test, y_pred_DT))
print(confusion_matrix(y_test, y_pred_DT))


import matplotlib.pyplot as plt

# Define the data
models = ['Random Forest', 'K Nearest Neighbors', 'Decision Tree', 'Logistic Regression', 'Gradient Boost']
accuracy = [0.95, 0.95, 0.91, 0.95, 0.95]

plt.figure(figsize=(8, 5))
plt.barh(models, accuracy, color='royalblue')
plt.xlabel('Accuracy')
plt.ylabel('Models')
plt.title('Average CV Mean Accuracy')
plt.xlim(0, 1)
plt.tight_layout()
plt.show()




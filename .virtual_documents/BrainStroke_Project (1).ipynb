import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from statistics import mean, stdev
from sklearn import preprocessing
from xgboost import XGBClassifier


#Ignoring future warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


# Load dataset
data = pd.read_csv("brain_stroke.csv")
data.head(5)





missing_values = data.isnull().sum()
print(missing_values)





# Check data types of each column
print(data.dtypes)


# Rename and clean all column titles
data.columns = data.columns.str.replace('_', ' ').str.title().str.replace(' ', '')
data.head()


# Encoding categorical variables
label_cols = ['Gender', 'EverMarried', 'WorkType', 'ResidenceType', 'SmokingStatus']
for col in label_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])


data


numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
min_values = data[numerical_cols].min()
max_values = data[numerical_cols].max()

# Calculate the range
range_values = max_values - min_values

# Combine results into a DataFrame for better visualization
range_df = pd.DataFrame({
    'Min': min_values,
    'Max': max_values,
    'Range': range_values
})

print(range_df)


# Feature Scaling (for continuous variables)
scaler = StandardScaler()
data[['Age', 'AvgGlucoseLevel', 'Bmi']] = scaler.fit_transform(data[['Age', 'AvgGlucoseLevel', 'Bmi']])


# Descriptive statistics for numerical features
numerical_cols = data.select_dtypes(include=['int64', 'float64']).columns.tolist()
print("Descriptive statistics for numerical features:")
print(data[numerical_cols].describe())

# Visualizing distributions of numerical features
for col in numerical_cols:
    plt.figure(figsize=(8, 5))
    sns.histplot(data[col], bins=30, kde=True)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.show()


# Check for imbalance in the target variable
plt.figure(figsize=(8, 5))
sns.countplot(x='Stroke', data=data)
plt.title("Distribution of Stroke")
plt.xlabel("Stroke")
plt.ylabel("Count")
plt.xticks([0, 1], ['No Stroke', 'Stroke'])
plt.show()


# Dropping stroke
X = data.drop('Stroke', axis=1)
y = data['Stroke']


X





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
lr = LogisticRegression()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    lr.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(lr.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
kc = KNeighborsClassifier()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    kc.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(kc.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
Rc = RandomForestClassifier()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    Rc.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(Rc.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
Gc = GradientBoostingClassifier()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    Gc.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(Gc.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Create Classifier Object
DT = DecisionTreeClassifier()

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    DT.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(DT.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))





# Feature Scaling for Input Features
scaler = preprocessing.MinMaxScaler()
X_scaled = scaler.fit_transform(X)

# Turning off warning
warnings.filterwarnings(action='ignore', category=UserWarning, module='xgboost')

# Create Classifier Object
xgb = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=1)

# Create StratifiedKFold Object
skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=1)
lst_accu_stratified = []

# Stratified K-Fold Cross Validation
for train_index, test_index in skf.split(X_scaled, y):
    X_train_fold, X_test_fold = X_scaled[train_index], X_scaled[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    # Fit the model on the training fold
    xgb.fit(X_train_fold, y_train_fold)

    # Append the accuracy of the fold
    lst_accu_stratified.append(xgb.score(X_test_fold, y_test_fold))

# Print the output for cross-validation
print('List of possible accuracy:', lst_accu_stratified)
print('\nMaximum Accuracy That can be obtained from this model is:', max(lst_accu_stratified)*100, '%')
print('\nMinimum Accuracy:', min(lst_accu_stratified)*100, '%')
print('\nOverall Accuracy:', mean(lst_accu_stratified)*100, '%')
print('\nStandard Deviation is:', stdev(lst_accu_stratified))


# KNN Grid search
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, accuracy_score
# Load a sample dataset (Brain Stroke dataset in this case)
X = data.drop(columns='Stroke')
y = data['Stroke']
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create a KNN classifier
knn = KNeighborsClassifier()
# Set the parameters for grid search
param_grid = {
    'n_neighbors': np.arange(1, 31),   # Number of neighbors
    'weights': ['uniform', 'distance'], # Weighting function
    'leaf_size': [10, 50, 100, 500]
}
# Initialize GridSearchCV
grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
# Fit the grid search to the data
grid_search.fit(X_train, y_train)
# Get the best parameters and estimator
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best Parameters:", best_params)
# Make predictions with the best estimator
y_pred = best_estimator.predict(X_test)
# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

grid_tuned_model = KNeighborsClassifier()
grid_clf = GridSearchCV(grid_tuned_model, param_grid, verbose=3)
grid_clf.fit(X_train, y_train)


# DecisionTreeClassifier Grid search
# Load a sample dataset (Brain Stroke dataset in this case)
X = data.drop(columns='Stroke')
y = data['Stroke']
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create a KNN classifier
dtc = DecisionTreeClassifier()
# Set the parameters for grid search
param_grid = {
    'max_depth': [None, 5, 10, 15, 20],       # Maximum depth of the tree
    'min_samples_split': [2, 5, 10],          # Minimum number of samples required to split an internal node
    'min_samples_leaf': [1, 2, 4],            # Minimum number of samples required to be at a leaf node
    'class_weight': [None, 'balanced'],       # Weights associated with classes
    'criterion': ['gini', 'entropy']          # Function to measure the quality of a split
}
# Initialize GridSearchCV
grid_search = GridSearchCV(dtc, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
# Fit the grid search to the data
grid_search.fit(X_train, y_train)
# Get the best parameters and estimator
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best Parameters:", best_params)
# Make predictions with the best estimator
y_pred = best_estimator.predict(X_test)
# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

grid_tuned_model = DecisionTreeClassifier()
grid_clf = GridSearchCV(grid_tuned_model, param_grid, verbose=3)
grid_clf.fit(X_train, y_train)


# RandomForest Grid search
# Load a sample dataset (Brain Stroke dataset in this case)
X = data.drop(columns='Stroke')
y = data['Stroke']
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create a KNN classifier
rfc = RandomForestClassifier()
# Set the parameters for grid search
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2],
    'criterion': ['gini', 'entropy']
}
# Initialize GridSearchCV
grid_search = GridSearchCV(rfc, param_grid, cv=5, scoring='accuracy', n_jobs=-1)
# Fit the grid search to the data
grid_search.fit(X_train, y_train)
# Get the best parameters and estimator
best_params = grid_search.best_params_
best_estimator = grid_search.best_estimator_
print("Best Parameters:", best_params)
# Make predictions with the best estimator
y_pred = best_estimator.predict(X_test)
# Evaluate the model
print("Accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))

grid_tuned_model = RandomForestClassifier()
grid_clf = GridSearchCV(grid_tuned_model, param_grid, verbose=3)
grid_clf.fit(X_train, y_train)


import matplotlib.pyplot as plt

# Define the data
models = ['Random Forest', 'K Nearest Neighbors', 'Decision Tree', 'Logistic Regression', 'Gradient Boost', 'XGBoost']
accuracy = [0.95, 0.95, 0.91, 0.95, 0.95, 0.93]

plt.figure(figsize=(8, 5))
plt.barh(models, accuracy, color='royalblue')
plt.xlabel('Accuracy')
plt.ylabel('Models')
plt.title('Average CV Mean Accuracy')
plt.xlim(0, 1)
plt.tight_layout()
plt.show()







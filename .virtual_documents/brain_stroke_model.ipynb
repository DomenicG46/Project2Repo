#Importing dependencies
import numpy as np
import warnings as warnings
import pandas as pd
import matplotlib.pyplot as plt
import hvplot.pandas
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler, LabelEncoder 
from sklearn.linear_model import LogisticRegression
from sklearn import svm 
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics
from sklearn.naive_bayes import GaussianNB 
from sklearn.metrics import accuracy_score , classification_report
warnings.filterwarnings('ignore')


#Ignoring future warnings
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)


#Loading in the csv as df_bs, where bs stands for brain stroke ;)
df_bs =pd.read_csv(r"brain_stroke.csv")


#Checking for how many patients are in the dataframe and how many 
df_bs.shape 


#Printing out the first 5 rows
df_bs.head(5)


#Checking the data type for all 11 columns
df_bs.info()


#Checking for duplicated values.
df_bs.duplicated().sum()


#Checking for null values
df_bs.isnull().sum()


#Checking how many patients had a stroke versus not.
df_bs['stroke'].value_counts()


#Data visualizations
#Setting a variable numerical_data that contains all the numerical data
numerical_data=df_bs[['age','avg_glucose_level','bmi']]
numerical_data


#Setting a variable categorical_data that contains all the categorical data
categorical_data=df_bs.select_dtypes(include=['object','int64'])
categorical_data


#Basic visualizations of columns by bar-graph
fig,ax=plt.subplots(3,3,figsize=(20,20))
fig.suptitle('Numerical data Visualization',fontsize=20)
for i,col in enumerate(categorical_data.columns):
    axs=ax[i//3,i%3]
    sns.countplot(x=df_bs[col],ax=axs)
    axs.set_title(col)
    plt.tight_layout()
plt.show()


#Looking at the age distribution
plt.figure(figsize=(8, 5))
palette = sns.color_palette("Set2")
sns.histplot(df_bs['age'], kde=True, color=palette[1])
plt.xticks(rotation=90, ha="right")
plt.show()


#Looking at residence type and stroke versus no stroke
plt.figure(figsize=(12,8))
palette = sns.color_palette("Set2")
sns.displot(data=df_bs, x="Residence_type", hue="stroke",bins=20, palette=palette)
plt.xticks(rotation=45, ha="right")


my_palette = ["#FF62D2", "#43AEF4"]

plt.figure(figsize=(10,5))

sns.displot(data=df_bs, x="age", hue="stroke", kind="kde",palette=my_palette)


plt.figure(figsize=(20,10))
palette = sns.color_palette(["#BA0ADA" , "#E20C7E"])
sns.displot(data=df_bs, x="age", hue="stroke",bins=20, palette=palette)
plt.xticks(rotation=45, ha="right")


numerical_data = df_bs[['age','avg_glucose_level','bmi']]
sns.kdeplot(data=numerical_data)


plt.figure(figsize=(12,8))
palette = sns.color_palette("Set2")
sns.displot(data=df_bs, x="work_type", hue="stroke",bins=20, palette=palette)
plt.xticks(rotation=45, ha="right")


['formerly smoked' 'never smoked' 'smokes' 'Unknown']
from sklearn.preprocessing import LabelEncoder
le=LabelEncoder()
for col in df_bs.columns:
    if df_bs[col].dtype=='object':
        df_bs[col]=le.fit_transform(df_bs[col])


df_bs.head()


from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
for col_name in df_bs.columns:
    if df_bs[col_name].nunique() > 5:
        df_bs[col_name] = scaler.fit_transform(df_bs[[col_name]])


X = df_bs.drop("stroke",axis=1)
y =df_bs['stroke']


from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,shuffle=True)


logreg = LogisticRegression()
logreg.fit(X_train, y_train)
y_pred = logreg.predict(X_test)

print('Logistic Regression accuracy score with all the features: {0:0.4f}'. format(accuracy_score(y_test, y_pred)*100))
print(classification_report(y_test, y_pred))


dec_clf=DecisionTreeClassifier(criterion='gini',random_state=3,max_depth=5)
dec_clf.fit(X_train,y_train)
y_pred=dec_clf.predict(X_test)
print('DecisionTreeClassifier accuracy score:', dec_clf.score(X_test, y_test)*100)
print(classification_report(y_test, y_pred))


from sklearn.ensemble import RandomForestClassifier
rfc=RandomForestClassifier(n_estimators=10,random_state=11)
rfc.fit(X_test, y_test)


rfc_pred = rfc.predict(X_test)
rfc_pred

from sklearn.metrics import accuracy_score
ac1 = accuracy_score(y_test, rfc_pred) * 100
print(classification_report(y_test, y_pred))


from sklearn.neighbors import KNeighborsClassifier
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create a KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)
# Fit the model
knn.fit(X_train, y_train)
# Make predictions
y_pred = knn.predict(X_test)
# Evaluate the model
accuracy = accuracy_score(y_test, y_pred) * 100
print("KNeightborsClassifier Accuracy:", accuracy)
print(classification_report(y_test, y_pred))


# Necessary library
from sklearn.ensemble import GradientBoostingClassifier
# Split the data into training and testing 
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Create a Gradient Boosting Classifier
gbc = GradientBoostingClassifier(random_state=42)
# Fit the model
gbc.fit(X_train, y_train)
# Make predictions
y_pred = gbc.predict(X_test)
# Model evaluation
accuracy = accuracy_score(y_test, y_pred) * 100
print("Gradient Boosting Classifier Accuracy:", accuracy)
print(classification_report(y_test, y_pred))



